{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from Libs.myFontLib import *\r\n",
    "from Libs.myFontData import *\r\n",
    "from Libs.myTrain import *\r\n",
    "from EfficientNet.model import EfficientNetEncoder\r\n",
    "from EfficientNet.utils import *\r\n",
    "from Libs.myLoss import *\r\n",
    "%matplotlib inline\r\n",
    "fontsInfoFile = \"fontsInfo.pkl\"\r\n",
    "import torch\r\n",
    "import gc\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import os\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import torch.nn.functional as F\r\n",
    "import shutil\r\n",
    "from torchinfo import summary\r\n",
    "from StyleGAN.network import *\r\n",
    "from Libs.mypSp import *\r\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "forCharaTrain = True\r\n",
    "forStyleTrain = False\r\n",
    "modelLevel = 3\r\n",
    "\r\n",
    "batchSize = 16 \r\n",
    "workers = 2\r\n",
    "\r\n",
    "d_dropout_limit = 0.75\r\n",
    "d_dropout = 0.925\r\n",
    "\r\n",
    "useKanji = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "compatibleDict = []\r\n",
    "with open(\"checker.pkl\", \"br\") as f:\r\n",
    "    compatibleDict = pickle.load(f)\r\n",
    "fixedDataset = []\r\n",
    "with open(\"fixedDataset.pkl\", \"br\") as f:\r\n",
    "    fixedDataset = pickle.load(f)\r\n",
    "styleDict = []\r\n",
    "with open(\"styleChecker.pkl\", \"br\") as f:\r\n",
    "    styleDict = pickle.load(f)\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(\"使用デバイス：\", device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "使用デバイス： cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "trainDataset = FontGeneratorDataset(FontTools(useKanji=useKanji), compatibleDict, [3, 3], styleDict, useTensor=True, startInd=10, augmentationP = [0.3, 0.3, 0], originalAugmentationP = [0.02, 0.05, 0.02, 0.04, 0.02, 0.05])\r\n",
    "validDataset = FontGeneratorDataset(FontTools(useKanji = useKanji), compatibleDict, [5, 5], styleDict, useTensor=True, startInd=0,\\\r\n",
    "      indN=10, isForValid=fixedDataset)\r\n",
    "\r\n",
    "trainDataLoader = torch.utils.data.dataloader.DataLoader(trainDataset,\\\r\n",
    "     batch_sampler=MyPSPBatchSampler(batchSize, trainDataset, japaneseRate=0.7), num_workers=workers, pin_memory=True)\r\n",
    "validDataLoader = torch.utils.data.dataloader.DataLoader(validDataset, batch_size=batchSize, num_workers=workers, pin_memory=True)\r\n",
    "charaList = []\r\n",
    "with open(\"Libs/difficult_list2.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
    "    line = f.readline()\r\n",
    "    while line:\r\n",
    "        charaList.append(line.strip())\r\n",
    "        line = f.readline()\r\n",
    "charaTrainDataset = MyPSPCharaDataset(charaList)\r\n",
    "charaDataLoader = torch.utils.data.dataloader.DataLoader(charaTrainDataset, batch_size = batchSize, shuffle = True, num_workers=workers, pin_memory=True)\r\n",
    "myPSP = MyPSP(ver=4, dropout_p=0.0, useBNform2s=True, useBin = True)\r\n",
    "myPSP.chara_encoder.init_original_layer()\r\n",
    "myPSP.style_encoder.init_original_layer()\r\n",
    "myPSP.set_for_chara_training(False)\r\n",
    "gen_settings = get_setting_json()\r\n",
    "discriminator = Discriminator4(dropout_p=d_dropout)\r\n",
    "charaDiscriminator = CharaDiscriminator(ver = 4)\r\n",
    "styleDiscriminator = StyleDiscriminator()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def trainModel(myPSP, D, charaDis, styleDis, dataLoaders, epochN, writer: SummaryWriter, forCharaTraining = False, forStyleTraining = False,\r\n",
    "     inheritOnlyModel = False,  checkpointFile = \"out.cpt\", checkpointFormat = \"cpts/output{}.cpt\", useFakeBackLog = False,\r\n",
    "      lookIntermidiate = False, charaDisCheckpointFile = \"\", dCheck = \"\", nowDropout = 0.0, changeDropout = False, checkGradNow = False):\r\n",
    "    trainCharaAndCharaDis = False\r\n",
    "    trainCharaDis = forCharaTraining\r\n",
    "    emergencySave = False # バランスが乱れた際に緊急セーブをしたか\r\n",
    "    forUnderTraining = forCharaTraining or forStyleTraining\r\n",
    "    torch.cuda.empty_cache()\r\n",
    "    gc.collect()\r\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "    print(\"使用デバイス：\", device)\r\n",
    "\r\n",
    "    dropoutChangeCount = 0\r\n",
    "    firstTeacherSize = 4 # 最初に読み込むテンソルが大きいとエラー落ちするため，制限\r\n",
    "\r\n",
    "    useWSGradient = True\r\n",
    "    useDforG = True\r\n",
    "    noiseP = 0.0\r\n",
    "\r\n",
    "    trainRate = 5\r\n",
    "    trainRateC = 0 # 0\r\n",
    "\r\n",
    "\r\n",
    "    SquareLossFactor = 10\r\n",
    "    fakeRawFactor = 0.002\r\n",
    "    charaDisFactor = 1000 # 100\r\n",
    "    styleLossFactor = 5\r\n",
    "    DforGFactor = 40\r\n",
    "    d_optimFun = torch.optim.AdamW\r\n",
    "    torch.backends.cudnn.benchmark = True\r\n",
    "    useFakeBackLog = (not forUnderTraining) and useFakeBackLog\r\n",
    "\r\n",
    "    modelsList = [myPSP, D, styleDis, charaDis]\r\n",
    "    optimizer_d_lr = 3e-5\r\n",
    "    optimizersList = getOptimizers(modelsList, forCharaTraining, trainCharaDis, d_optimFun, optimizer_d_lr)\r\n",
    "    optimizer, optimizer_d, optimizer_styleDis, optimizer_charaDis = optimizersList\r\n",
    "\r\n",
    "    charaDisLoss = torch.nn.MSELoss()\r\n",
    "    myPSP.to(device)\r\n",
    "\r\n",
    "    myPSP.train()\r\n",
    "    if(trainCharaDis or not forUnderTraining):\r\n",
    "        charaDis.to(device)\r\n",
    "        charaDis.train()\r\n",
    "    styleDis.to(device)\r\n",
    "    styleDis.train()\r\n",
    "    if(not forUnderTraining):\r\n",
    "        D.to(device)\r\n",
    "        D.train()\r\n",
    "\r\n",
    "\r\n",
    "    start = loadCheckpoints(checkpointFile, modelsList, optimizersList, \\\r\n",
    "        dCheck, charaDisCheckpointFile,\\\r\n",
    "        inheritOnlyModel, forUnderTraining, trainCharaDis)\r\n",
    "    \r\n",
    "    genIntermidiateList = getGenIntermidiateLayers(myPSP)\r\n",
    "    disIntermidiateList = None\r\n",
    "    if not forUnderTraining: \r\n",
    "        disIntermidiateList = getDisIntermidiatelayers(D)\r\n",
    "    train_d_correct = 0\r\n",
    "\r\n",
    "    # epochのループ\r\n",
    "    for epoch in range(start, epochN):\r\n",
    "        trainD = True\r\n",
    "        epochStartTime = time.time()\r\n",
    "        epochGLoss = 0\r\n",
    "        epochDLoss = 0\r\n",
    "        print('-------------')\r\n",
    "        print('Epoch {}/{}'.format(epoch, epochN))\r\n",
    "\r\n",
    "        usingCharaDataLoader = False\r\n",
    "\r\n",
    "        for phase in [\"train\", \"val\"]:\r\n",
    "            epochCharaLoss = 0\r\n",
    "            GLossDict = initGLossDict()\r\n",
    "            epochGLoss = 0\r\n",
    "            epochDLoss = 0\r\n",
    "            epochtrainGDAcc = 0\r\n",
    "            epochtrainGn = 0\r\n",
    "            epochDLossList = np.zeros(3)\r\n",
    "            dataLoader = None\r\n",
    "            if phase== \"train\":\r\n",
    "                if epoch == 0:\r\n",
    "                    continue\r\n",
    "                myPSP.train()\r\n",
    "                # D.train()\r\n",
    "                dataLoader = dataLoaders[0]\r\n",
    "                if(forCharaTraining and random.random() > 0.2 and not trainCharaAndCharaDis):\r\n",
    "                    dataLoader = dataLoaders[2]\r\n",
    "                    usingCharaDataLoader = True\r\n",
    "                else:\r\n",
    "                    usingCharaDataLoader = False\r\n",
    "            else:\r\n",
    "                myPSP.eval()\r\n",
    "                # D.eval()\r\n",
    "                dataLoader = dataLoaders[1]\r\n",
    "                usingCharaDataLoader = False\r\n",
    "            print('---------')\r\n",
    "            print(\"({})\".format(phase))\r\n",
    "\r\n",
    "            # Discriminatorの正解率\r\n",
    "            discriminator_problems_n = discriminator_problems_n_b = 0 # 入力された回数\r\n",
    "            discriminator_correct_n =  discriminator_correct_n_b =  0# そのうちの正解数\r\n",
    "            TCorrectN = 0\r\n",
    "            if(epoch % 1 == 0 and useFakeBackLog and phase == \"train\"):\r\n",
    "                # FakesBackLogでDiscriminatorを再訓練\r\n",
    "                epochDLoss, fakesBackLog, nowFakesBackPath, scores = trainWithBackLog(phase, device, D, optimizer_d, noiseP, useWSGradient)\r\n",
    "                discriminator_problems_n_b, discriminator_correct_n_b = scores\r\n",
    "            gc.collect()\r\n",
    "            torch.cuda.empty_cache()\r\n",
    "            \r\n",
    "            iteration = 0\r\n",
    "            d_iteration = 0\r\n",
    "            for data in dataLoader:\r\n",
    "                minibatch_size = data[0][0].size()[0]\r\n",
    "                beforeCharacter = None\r\n",
    "                if usingCharaDataLoader:\r\n",
    "                    beforeCharacter = data\r\n",
    "                else: \r\n",
    "                    beforeCharacter =  data[0][0]\r\n",
    "                alpha = torch.ones((1, 1))\r\n",
    "                beforeCharacter =  beforeCharacter.to(device, torch.float32, non_blocking=True)\r\n",
    "                alpha = alpha.to(device, torch.float32, non_blocking=True)\r\n",
    "                afterCharacter = beforeCharacter\r\n",
    "                teachers = None\r\n",
    "                styleLabel  = None\r\n",
    "                # label_real = (torch.ones((minibatch_size, )) + 0.6 * (torch.rand((minibatch_size, )) - 0.5)).to(device)\r\n",
    "                # label_fake = (torch.zeros((minibatch_size, )) + 0.3 * torch.rand((minibatch_size, )) ).to(device)\r\n",
    "                if not forCharaTraining or trainCharaAndCharaDis:\r\n",
    "                    afterCharacter = data[0][1]\r\n",
    "                    teachers = data[1][:, :, 1] # teachers = data[1][:, :, 1]\r\n",
    "                    styleLabel = data[2]\r\n",
    "                    if(iteration <= 2):\r\n",
    "                        teachers = teachers[:, :firstTeacherSize] #最初に読み込むサイズを制限\r\n",
    "                    afterCharacter =  afterCharacter.to(device, torch.float32, non_blocking=True)\r\n",
    "                    \r\n",
    "                    teachers =  teachers.to(device, torch.float32, non_blocking=True)\r\n",
    "                    styleLabel = styleLabel.to(device, torch.float32, non_blocking=True)\r\n",
    "\r\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\r\n",
    "                    # Generator Loss\r\n",
    "\r\n",
    "                    # 中間層出力用の関数\r\n",
    "                    Dhandles, Ghandles =  getIntermidiateHandlers(genIntermidiateList, disIntermidiateList, writer, iteration, epoch, \\\r\n",
    "                            lookIntermidiate, checkGradNow,  forUnderTraining)\r\n",
    "                    \r\n",
    "                    factors = [SquareLossFactor, fakeRawFactor, styleLossFactor, charaDisFactor]\r\n",
    "                    iterGLoss, featureT, fakes = forwardG(myPSP, styleDis, charaDis, charaDisLoss, beforeCharacter, teachers, afterCharacter,\\\r\n",
    "                        alpha, styleLabel, GLossDict, factors, \\\r\n",
    "                        forCharaTraining, forStyleTraining)\r\n",
    "                    torch.cuda.empty_cache()\r\n",
    "                    gc.collect()\r\n",
    "                    \r\n",
    "                    if(lookIntermidiate):\r\n",
    "                        for handle in Ghandles:\r\n",
    "                            handle.remove()\r\n",
    "\r\n",
    "                    if(not forUnderTraining and useDforG):\r\n",
    "                        fakes = transforms.Normalize(FontGeneratorDataset.IMAGE_MEAN, FontGeneratorDataset.IMAGE_VAR)(fakes)\r\n",
    "                        beforeCharacterN, fakesN, teachersN = MyPSPAugmentation.getNoisedImages([beforeCharacter, fakes, teachers], noiseP,device)\r\n",
    "                        d_fake = D(fakesN, teachersN, alpha)\r\n",
    "                        if(lookIntermidiate):\r\n",
    "                            for handle in Dhandles:\r\n",
    "                                handle.remove()\r\n",
    "                        iterGLoss += DforGFactor * g_wgan_loss(d_fake)\r\n",
    "                        del beforeCharacterN, d_fake, fakesN, teachersN\r\n",
    "                    epochGLoss += iterGLoss.item()\r\n",
    "                    \r\n",
    "                    if phase == \"train\":\r\n",
    "                        iterGLoss.backward()\r\n",
    "                        del iterGLoss\r\n",
    "                        if(iteration == 0 and (epoch % 10 == 0 or checkGradNow)):\r\n",
    "                            writeGeneratorGradients(myPSP, writer, epoch, styleDis)\r\n",
    "                        optimizer.step()\r\n",
    "                        optimizer_styleDis.step()\r\n",
    "                        optimizer.zero_grad()\r\n",
    "                        optimizer_styleDis.zero_grad()\r\n",
    "                        charaDis.zero_grad()\r\n",
    "                        D.zero_grad()\r\n",
    "\r\n",
    "                    if(not forUnderTraining):\r\n",
    "                        fakes = fakes.detach() # Disctriminatorで使う\r\n",
    "                    if iteration <= 2 and not forStyleTraining:\r\n",
    "                        for i in range(2):\r\n",
    "                            image = None\r\n",
    "                            if forCharaTraining:\r\n",
    "                                image = torch.stack([beforeCharacter[i*3], fakes[i*3], beforeCharacter[i*3+1], fakes[i*3+1], \r\n",
    "                                                beforeCharacter[i*3+2], fakes[i*3+2]])\r\n",
    "                            else:\r\n",
    "                                image = torch.stack([beforeCharacter[i],\\\r\n",
    "                                    afterCharacter[i], 1-(fakes * FontGeneratorDataset.IMAGE_VAR + FontGeneratorDataset.IMAGE_MEAN )[i],\r\n",
    "                                    teachers[i, 0, :], teachers[i, 1, :]])\r\n",
    "                            writer.add_images(\"{}/{}\".format(phase, i+iteration*2), image, global_step=epoch)\r\n",
    "                            del image\r\n",
    "                    \r\n",
    "                    torch.cuda.empty_cache()\r\n",
    "                    gc.collect()\r\n",
    "\r\n",
    "                    \r\n",
    "                    # 以下D\r\n",
    "                    if(trainCharaDis):\r\n",
    "                        featureO = charaDis(afterCharacter)\r\n",
    "                        c_loss = charaDisLoss(featureO, featureT)\r\n",
    "                        epochCharaLoss += c_loss.item()\r\n",
    "                        if phase == \"train\":\r\n",
    "                            c_loss.backward()\r\n",
    "                            optimizer_charaDis.step()\r\n",
    "                            optimizer_charaDis.zero_grad()\r\n",
    "                        del featureT, featureO, c_loss\r\n",
    "                    \r\n",
    "                    if(not forUnderTraining and trainD and (iteration % trainRate == trainRateC or phase == \"val\")):\r\n",
    "                        fakesN, afterCharacterN, teachersN =\\\r\n",
    "                             MyPSPAugmentation.getNoisedImages([fakes, afterCharacter, teachers], noiseP, device)\r\n",
    "                        if(phase == \"train\"):\r\n",
    "                            # ここでメモリをよく使うため，minibatchを小さくしておく\r\n",
    "                            if(useWSGradient):\r\n",
    "                                if(minibatch_size > 2):\r\n",
    "                                    minibatch_size = 2\r\n",
    "                                else:\r\n",
    "                                    minibatch_size = 1\r\n",
    "                                if(teachersN.shape[1] >= 4):\r\n",
    "                                    teachersN = teachersN[:, : 3]\r\n",
    "                            else:\r\n",
    "                                minibatch_size = 4\r\n",
    "                            # beforeCharacterN = beforeCharacterN[:minibatch_size]\r\n",
    "                            fakesN = fakesN[:minibatch_size]\r\n",
    "                            afterCharacterN = afterCharacterN[:minibatch_size]\r\n",
    "                            teachersN = teachersN[:minibatch_size]\r\n",
    "\r\n",
    "                        d_loss, discCorrectN, lossList, tcorrect, fcorrect = d_wgan_loss2(D, None, afterCharacterN,\\\r\n",
    "                             fakesN, teachersN, alpha, phase, useGradient=useWSGradient, useBefore=False)\r\n",
    "                        TCorrectN += tcorrect\r\n",
    "                        epochDLossList += lossList\r\n",
    "                        epochDLoss += d_loss.item()\r\n",
    "                        discriminator_problems_n += minibatch_size*2\r\n",
    "                        discriminator_correct_n += discCorrectN\r\n",
    "                        del  afterCharacterN, teachersN,  fakesN\r\n",
    "\r\n",
    "                        if phase == \"train\":\r\n",
    "                            d_loss.backward()\r\n",
    "                            del d_loss\r\n",
    "                            if(d_iteration == 0 and (epoch % 10 == 0 or checkGradNow)):\r\n",
    "                                writeDiscriminatorGradients(D, writer, epoch)\r\n",
    "                            optimizer_d.step()\r\n",
    "                            optimizer_d.zero_grad()               \r\n",
    "                        d_iteration += 1      \r\n",
    "\r\n",
    "                    iteration += 1\r\n",
    "                    print(\"\\riter {:4}/{}\".format(iteration, len(dataLoader)), end=\"\")\r\n",
    "                    # add to fakesBackLog\r\n",
    "                    if(epoch % 10 == 0 and phase == \"train\" and iteration == 1 and useFakeBackLog):\r\n",
    "                        beforeCharacter = beforeCharacter.cpu()\r\n",
    "                        afterCharacter = afterCharacter.cpu()\r\n",
    "                        fakes = fakes.cpu()\r\n",
    "                        teachers = teachers.cpu()\r\n",
    "                        fakesBackLog.append([beforeCharacter, afterCharacter, fakes, teachers])\r\n",
    "                    \r\n",
    "                    del beforeCharacter, afterCharacter, teachers, alpha, data, fakes\r\n",
    "\r\n",
    "\r\n",
    "            # epochのphaseごとのloss\r\n",
    "            if(d_iteration == 0):\r\n",
    "                d_loss = np.nan\r\n",
    "            else:\r\n",
    "                d_loss =  epochDLoss / d_iteration\r\n",
    "            g_loss =  epochGLoss / iteration\r\n",
    "            c_loss = epochCharaLoss / iteration\r\n",
    "            GLossDict = {key: GLossDict[key] / iteration for key in GLossDict}   \r\n",
    "\r\n",
    "            discriminator_ns = [[discriminator_correct_n , discriminator_problems_n],\r\n",
    "                                [discriminator_correct_n_b , discriminator_problems_n_b ]]\r\n",
    "            d_correct_rate = printResults(d_loss, discriminator_ns, g_loss, GLossDict, c_loss, epochDLossList, TCorrectN,  epochStartTime, \\\r\n",
    "                                epoch, forUnderTraining, trainD\r\n",
    "                                )\r\n",
    "            outputWriter(writer,d_loss, d_correct_rate, g_loss, GLossDict, c_loss, phase, epoch, \\\r\n",
    "                forUnderTraining, trainD)\r\n",
    "            if( phase == \"train\"): # 後でdropout率を更新するときに使う\r\n",
    "                train_d_correct = d_correct_rate\r\n",
    "        \r\n",
    "        if(os.path.exists(checkpointFile)):\r\n",
    "            shutil.copy(checkpointFile, \"cpts/before.cpt\")\r\n",
    "        if epoch % 20 == 0:\r\n",
    "            checkpointFile = checkpointFormat.format(epoch)\r\n",
    "        checkpoint = {\"epoch\": epoch, \r\n",
    "            \"modelStateDict\": myPSP.state_dict(), \r\n",
    "            \"discriminatorStateDict\": D.state_dict(),\r\n",
    "            \"charaDiscriminatorStateDict\": charaDis.state_dict(),\r\n",
    "            \"styleDiscriminatorStateDict\": styleDis.state_dict(),\r\n",
    "            \"optStateDict\": optimizer.state_dict(), \r\n",
    "            \"optDStateDict\": optimizer_d.state_dict(),\r\n",
    "            \"optCDStateDict\": optimizer_charaDis.state_dict() if trainCharaDis  else None,\r\n",
    "            \"optSDStateDict\": optimizer_styleDis.state_dict()\r\n",
    "            }\r\n",
    "        torch.save(checkpoint, checkpointFile)\r\n",
    "\r\n",
    "        D, optimizer_d,  disIntermidiateList, dropoutChangeCount, nowDropout, trainRate, trainRateC, emergencySave = \\\r\n",
    "            updateDropout(D, optimizer_d, disIntermidiateList, writer, checkpoint, checkpointFile, train_d_correct,\r\n",
    "                            nowDropout, dropoutChangeCount, epoch,\\\r\n",
    "                            trainRate,  trainRateC, device, d_optimFun, optimizer_d_lr,\\\r\n",
    "                            trainD, emergencySave, forUnderTraining, changeDropout)\r\n",
    "        \r\n",
    "        if(epoch % 3 == 0 and useFakeBackLog):\r\n",
    "            torch.save({FAKES_BACK_LOG_KEY: fakesBackLog}, nowFakesBackPath)\r\n",
    "    return \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "myPSP.set_level(modelLevel)\r\n",
    "myPSP.set_for_chara_training(forCharaTrain)\r\n",
    "myPSP.set_for_style_training(forStyleTrain)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "writer = SummaryWriter(log_dir=\"./logs1\")\r\n",
    "logs = trainModel(myPSP,discriminator, charaDiscriminator, styleDiscriminator, [trainDataLoader, validDataLoader, charaDataLoader], 100000, \\\r\n",
    "     writer, checkpointFile=\"cpts/output0.cpt\", useFakeBackLog=not (forStyleTrain or forCharaTrain), forCharaTraining = forCharaTrain, forStyleTraining = forStyleTrain, lookIntermidiate=False, \\\r\n",
    "     nowDropout=d_dropout, changeDropout=True, checkGradNow=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('PytorchGPU': conda)"
  },
  "interpreter": {
   "hash": "7b8778c506322cce5bb4c7fc5151e16e315ad2de24c3d496674e1c9af8a8a7a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}