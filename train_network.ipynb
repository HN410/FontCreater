{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from Libs.myFontLib import *\r\n",
    "from Libs.myFontData import *\r\n",
    "from EfficientNet.utils import *\r\n",
    "from StyleGAN.network import *\r\n",
    "from Libs.mypSp import *\r\n",
    "\r\n",
    "import torch\r\n",
    "import gc\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import os\r\n",
    "\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "fontsInfoFile = \"fontsInfo.pkl\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batchSize = 2\r\n",
    "compatibleDict = []\r\n",
    "with open(\"checker.pkl\", \"br\") as f:\r\n",
    "    compatibleDict = pickle.load(f)\r\n",
    "fixedDataset = []\r\n",
    "with open(\"fixedDataset.pkl\", \"br\") as f:\r\n",
    "    fixedDataset = pickle.load(f)\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(\"使用デバイス：\", device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainDataset = FontGeneratorDataset(FontTools(), compatibleDict, [5, 11], useTensor=True, startInd=10)\r\n",
    "validDataset = FontGeneratorDataset(FontTools(), compatibleDict, [5, 11], useTensor=True, startInd=0,\\\r\n",
    "      indN=10, isForValid=fixedDataset)\r\n",
    "\r\n",
    "trainDataLoader = torch.utils.data.dataloader.DataLoader(trainDataset,\\\r\n",
    "     batch_sampler=MyPSPBatchSampler(batchSize, trainDataset, japaneseRate=0.5))\r\n",
    "validDataLoader = torch.utils.data.dataloader.DataLoader(validDataset, batch_size=batchSize)\r\n",
    "myPSP = MyPSP()\r\n",
    "myPSP.chara_encoder.init_original_layer()\r\n",
    "myPSP.style_encoder.init_original_layer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def getUpdatedParams(myPSP):\r\n",
    "    GENERATOR_NAME = \"style_gen\"\r\n",
    "    ENCODER_CONV_NAME = \"encode_convs\"\r\n",
    "    ENCODER_MAP_NAME =  \"map2styles\"\r\n",
    "    paramUpdatedGen = []\r\n",
    "    paramUpdatedEncMain = []\r\n",
    "    paramUpdatedEncConvMap = []\r\n",
    "    for name, param in myPSP.named_parameters():\r\n",
    "        param.requires_grad = True\r\n",
    "        if GENERATOR_NAME in  name:\r\n",
    "            paramUpdatedGen.append(param)\r\n",
    "        else:\r\n",
    "            if(ENCODER_CONV_NAME in name or ENCODER_MAP_NAME in name):\r\n",
    "                paramUpdatedEncConvMap.append(param)\r\n",
    "            else:\r\n",
    "                paramUpdatedEncMain.append(param)\r\n",
    "    return [paramUpdatedGen, paramUpdatedEncMain, paramUpdatedEncConvMap]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trainModel(myPSP, dataLoaders, epochN, writer: SummaryWriter, checkpointFile = \"out.cpt\"):\r\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "    print(\"使用デバイス：\", device)\r\n",
    "\r\n",
    "    params = getUpdatedParams(myPSP)\r\n",
    "    optimizer = torch.optim.Adam([\r\n",
    "        {\"params\": params[0], \"lr\": 1e-3}, \r\n",
    "        {\"params\": params[1], \"lr\": 1e-3}, \r\n",
    "        {\"params\": params[2], \"lr\": 1e-3}\r\n",
    "    ], betas=(0.0, 0.99), eps = 1e-8)\r\n",
    "\r\n",
    "    myPSP.to(device)\r\n",
    "    myPSP.train()\r\n",
    "    torch.backends.cudnn.benchmark = True\r\n",
    "\r\n",
    "    trainLogs = []\r\n",
    "    validLogs = []\r\n",
    "    logs = [trainLogs, validLogs]\r\n",
    "    start = 0\r\n",
    "    if os.path.exists(checkpointFile):\r\n",
    "\r\n",
    "\r\n",
    "        checkpoint = torch.load(checkpointFile)\r\n",
    "        start = checkpoint[\"epoch\"]+1\r\n",
    "        myPSP.load_state_dict(checkpoint[\"modelStateDict\"])\r\n",
    "        optimizer.load_state_dict(checkpoint[\"optStateDict\"])\r\n",
    "        logs = checkpoint[\"logs\"]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    # epochのループ\r\n",
    "    for epoch in range(start, epochN):\r\n",
    "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\r\n",
    "        epochStartTime = time.time()\r\n",
    "        epochLoss = 0\r\n",
    "        print('-------------')\r\n",
    "        print('Epoch {}/{}'.format(epoch, epochN))\r\n",
    "\r\n",
    "\r\n",
    "        for phase in [\"train\", \"val\"]:\r\n",
    "            iteration = 0\r\n",
    "            dataLoader = None\r\n",
    "            if phase== \"train\":\r\n",
    "                if epoch == 0:\r\n",
    "                    continue\r\n",
    "                myPSP.train()\r\n",
    "                dataLoader = dataLoaders[0]\r\n",
    "            else:\r\n",
    "                myPSP.eval()\r\n",
    "                dataLoader = dataLoaders[1]\r\n",
    "            print('---------')\r\n",
    "            print(\"({})\".format(phase))\r\n",
    "\r\n",
    "            for data in dataLoader:\r\n",
    "                beforeCharacter = data[0][0]\r\n",
    "                afterCharacter = data[0][1]\r\n",
    "                teachers = data[1]\r\n",
    "                alpha = torch.ones((1, 1))\r\n",
    "                beforeCharacter =  beforeCharacter.to(device, torch.float32)\r\n",
    "                afterCharacter =  afterCharacter.to(device, torch.float32)\r\n",
    "                teachers =  teachers.to(device, torch.float32)\r\n",
    "                alpha = alpha.to(device, torch.float32)\r\n",
    "\r\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\r\n",
    "                    fakes = myPSP(beforeCharacter, teachers, alpha)\r\n",
    "                    loss = MyPSPLoss(onSharp=0.3)(fakes, afterCharacter)\r\n",
    "\r\n",
    "                    if iteration == 0:\r\n",
    "                        for i in range(2):\r\n",
    "                            writer.add_images(\"{}/{}\".format(phase, i), torch.stack([beforeCharacter[i],\\\r\n",
    "                             afterCharacter[i], fakes[i], teachers[i, 0, 1, :]]), global_step=epoch)\r\n",
    "\r\n",
    "\r\n",
    "                    if phase == \"train\":\r\n",
    "                        optimizer.zero_grad()\r\n",
    "                        loss.backward()\r\n",
    "                        optimizer.step()\r\n",
    "\r\n",
    "                    iteration += 1\r\n",
    "                    epochLoss += loss\r\n",
    "                    \r\n",
    "                    del beforeCharacter, afterCharacter, teachers, alpha, data, fakes, loss\r\n",
    "                    torch.cuda.empty_cache()\r\n",
    "                    gc.collect()\r\n",
    "\r\n",
    "            # epochのphaseごとのloss\r\n",
    "            loss =  epochLoss.item() / iteration\r\n",
    "            if(phase == \"train\"):\r\n",
    "                logs[0].append(loss)\r\n",
    "                writer.add_scalar(\"loss/train\", loss, global_step=epoch)\r\n",
    "            else:\r\n",
    "                logs[1].append(loss)\r\n",
    "                writer.add_scalar(\"loss/valid\", loss, global_step=epoch)\r\n",
    "            epochFinishTime = time.time()\r\n",
    "            print('-----')\r\n",
    "            print('epoch {} || Epoch_Loss:{:.4f} '.format(\r\n",
    "                epoch, loss))\r\n",
    "            print('timer:  {:.4f} sec.'.format(epochFinishTime - epochStartTime))\r\n",
    "\r\n",
    "        checkpoint = {\"epoch\": epoch, \r\n",
    "            \"modelStateDict\": myPSP.state_dict(), \r\n",
    "            \"optStateDict\": optimizer.state_dict(), \r\n",
    "            \"logs\": logs}\r\n",
    "        torch.save(checkpoint, checkpointFile)\r\n",
    "    return logs\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "writer = SummaryWriter(log_dir=\"./logs2\")\r\n",
    "logs = trainModel(myPSP, [trainDataLoader, validDataLoader], 1000, writer)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}